{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21758aa2",
   "metadata": {},
   "source": [
    "# Workspace Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184ac76d",
   "metadata": {},
   "source": [
    "## Some Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46437eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.utils import resample\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from matplotlib import pyplot as plt\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, EarlyStoppingCallback, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import time\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04486a7b",
   "metadata": {},
   "source": [
    "## Some Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aaf3447",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 256\n",
    "RANDOM_SEED = 42\n",
    "VAL_SIZE = 0.15\n",
    "TEST_SIZE = 0.15\n",
    "RESULTS_DIR = './results'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af97d37",
   "metadata": {},
   "source": [
    "## Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4046cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stars_to_sentiment(stars):\n",
    "    if stars <= 1:\n",
    "        return \"Negative\"\n",
    "    elif stars == 2:\n",
    "        return \"Neutral\"\n",
    "    else:\n",
    "        return \"Positive\"\n",
    "\n",
    "################### transformers ######################\n",
    "# Tokenizer \n",
    "def tokenize_reviews(df, tokenizer, max_length=512):\n",
    "    encodings = tokenizer(\n",
    "        df[\"Review\"].tolist(),\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    labels = torch.tensor(df[\"Rating\"].values)\n",
    "    return encodings, labels\n",
    "\n",
    "# Unpack the encodings (input_ids and attention_mask)\n",
    "class EncodedDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.encodings['input_ids'][idx],\n",
    "            'attention_mask': self.encodings['attention_mask'][idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    prec, rec, f_score, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
    "    return {\"precision\": prec,\n",
    "            \"recall\": rec,\n",
    "            \"f1\": f_score}\n",
    "    \n",
    "################### LSTM ######################\n",
    "\n",
    "# Encode sentences with padding and OOV handling\n",
    "def encode_sentence(tokens, vocab, max_len):\n",
    "    encoded = [vocab.get(word, vocab[\"<OOV>\"]) for word in tokens]\n",
    "    padded = encoded[:max_len] + [vocab[\"<PAD>\"]] * max(0, max_len - len(encoded))\n",
    "    return padded\n",
    "\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.long)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440283ec",
   "metadata": {},
   "source": [
    "## Check whether the GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8d3f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2691817e",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80eca058",
   "metadata": {},
   "source": [
    "### Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa79168",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = kagglehub.dataset_download(\"andrewmvd/trip-advisor-hotel-reviews\")\n",
    "\n",
    "print(\"Path to dataset files:\", dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81986c8",
   "metadata": {},
   "source": [
    "### Dataloading on Pandas + preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c89b598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset CSV file\n",
    "df = pd.read_csv(f\"{dataset_path}/tripadvisor_hotel_reviews.csv\")\n",
    "df[\"Rating\"] = df[\"Rating\"] - 1 # Convert 1–5 to 0–4\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# removing html tags, converting to lowercase and removing stop words\n",
    "df[\"Review\"] = df[\"Review\"].str.lower()\n",
    "df[\"Review\"] = df[\"Review\"].str.replace(r\"<.*?>\", \"\", regex=True)\n",
    "df[\"Review\"] = df[\"Review\"].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in stop_words]))\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# Add a column with the number of tokens per review\n",
    "token_counts = tokenizer(\n",
    "    df[\"Review\"].tolist(),\n",
    "    padding=False,\n",
    "    truncation=False,\n",
    "    return_length=True\n",
    ")[\"length\"]\n",
    "df[\"token_count\"] = token_counts\n",
    "\n",
    "# Drop reviews with token_count > MAX_LEN\n",
    "df_filtered = df[df[\"token_count\"] <= MAX_LEN].reset_index(drop=True)\n",
    "\n",
    "# # Oversample classes 0, 1, 2, 3 to match class 4's count\n",
    "# max_count = df_filtered[df_filtered[\"Rating\"] == 4].shape[0]\n",
    "# dfs = []\n",
    "# for label in range(5):\n",
    "#     df_class = df_filtered[df_filtered[\"Rating\"] == label]\n",
    "#     if label < 4:\n",
    "#         df_class_upsampled = resample(\n",
    "#             df_class,\n",
    "#             replace=True,\n",
    "#             n_samples=max_count,\n",
    "#             random_state=RANDOM_SEED\n",
    "#         )\n",
    "#         dfs.append(df_class_upsampled)\n",
    "#     else:\n",
    "#         dfs.append(df_class)\n",
    "# df_filtered = pd.concat(dfs).sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "\n",
    "# # Undersample classes 1, 2, 3, 4 to match class 0's count\n",
    "# min_count = df_filtered[df_filtered[\"Rating\"] == 0].shape[0]\n",
    "# dfs = []\n",
    "# for label in range(5):\n",
    "#     df_class = df_filtered[df_filtered[\"Rating\"] == label]\n",
    "#     if label > 0:\n",
    "#         df_class_downsampled = resample(\n",
    "#             df_class,\n",
    "#             replace=False,\n",
    "#             n_samples=min_count,\n",
    "#             random_state=RANDOM_SEED\n",
    "#         )\n",
    "#         dfs.append(df_class_downsampled)\n",
    "#     else:\n",
    "#         dfs.append(df_class)\n",
    "# df_filtered = pd.concat(dfs).sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "\n",
    "# Check new distribution\n",
    "print(\"New dataset size:\", len(df_filtered))\n",
    "print(df_filtered[\"token_count\"].describe())\n",
    "\n",
    "print(df_filtered[\"Rating\"].value_counts())\n",
    "\n",
    "# Split off the test set\n",
    "df_temp, df_test = train_test_split(df_filtered, test_size=TEST_SIZE, random_state=RANDOM_SEED, shuffle=True)\n",
    "\n",
    "# Split the remaining into train and validation\n",
    "df_train, df_val = train_test_split(df_temp, test_size=VAL_SIZE, random_state=RANDOM_SEED, shuffle=True)\n",
    "\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "\n",
    "print(f\"Train size: {len(df_train)}\")\n",
    "print(f\"Validation size: {len(df_val)}\")\n",
    "print(f\"Test size: {len(df_test)}\")\n",
    "\n",
    "print(df_filtered.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35ea0c5",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8377fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "rating_counts = df_filtered['Rating'].value_counts().sort_index()\n",
    "# Offset labels by 1 for display (since ratings are 0–4, but original scale is 1–5)\n",
    "labels_offset = [i + 1 for i in rating_counts.index]\n",
    "plt.bar(labels_offset, rating_counts.values, color='skyblue', edgecolor='black')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Ratings in Filtered Dataset')\n",
    "plt.xticks(labels_offset)\n",
    "for i, count in zip(labels_offset, rating_counts.values):\n",
    "    plt.text(i, count + 50, str(count), ha='center', va='bottom')\n",
    "\n",
    "# Sentiment distribution \n",
    "plt.subplot(1, 2, 2)\n",
    "df_filtered['Sentiment'] = df_filtered['Rating'].apply(stars_to_sentiment)\n",
    "sentiment_counts = df_filtered['Sentiment'].value_counts()\n",
    "colors = ['red', 'orange', 'green']\n",
    "plt.pie(sentiment_counts.values, labels=sentiment_counts.index, autopct='%1.1f%%', colors=colors)\n",
    "plt.title('Sentiment Distribution in Filtered Dataset')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('report/images/label_distribution.png')\n",
    "plt.show()\n",
    "\n",
    "# Word Cloud\n",
    "# Combine all reviews into one text\n",
    "all_text = ' '.join(df_filtered['Review'].values)\n",
    "\n",
    "# Generate word cloud\n",
    "plt.figure(figsize=(12, 8))\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=100).generate(all_text)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud of Hotel Reviews', fontsize=16, fontweight='bold')\n",
    "plt.savefig('report/images/wordcloud.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca39a880",
   "metadata": {},
   "source": [
    "### Torch Dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b82438",
   "metadata": {},
   "source": [
    "#### RoBERTa dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73c358c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize train, validation, and test sets\n",
    "train_encodings = tokenize_reviews(df_train, tokenizer)\n",
    "val_encodings = tokenize_reviews(df_val, tokenizer)\n",
    "test_encodings = tokenize_reviews(df_test, tokenizer)\n",
    "\n",
    "# Build datasets\n",
    "train_dataset = EncodedDataset(*train_encodings)\n",
    "val_dataset = EncodedDataset(*val_encodings)\n",
    "test_dataset = EncodedDataset(*test_encodings)\n",
    "\n",
    "# Only keep the first 100 entries for each dataset\n",
    "mock_dataset = EncodedDataset(\n",
    "    {k: v[:100] for k, v in train_encodings[0].items()},\n",
    "    train_encodings[1][:100]\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader_bert = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader_bert = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader_bert = DataLoader(test_dataset, batch_size=32)\n",
    "mock_loader_bert = DataLoader(mock_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94ee7d5",
   "metadata": {},
   "source": [
    "#### LSTM dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c63e8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df_filtered[\"Review\"].values\n",
    "print(\"Number of reviews:\", len(texts))\n",
    "labels = df_filtered[\"Rating\"].values\n",
    "\n",
    "# Tokenize and build vocabolary\n",
    "tokenized_texts = [word_tokenize(text.lower()) for text in texts]\n",
    "vocab = Counter([word for sentence in tokenized_texts for word in sentence])\n",
    "vocab = {word: i+2 for i, (word, _) in enumerate(vocab.most_common(10000))}\n",
    "vocab[\"<PAD>\"] = 0\n",
    "vocab[\"<OOV>\"] = 1\n",
    "\n",
    "encoded_texts = [encode_sentence(tokens, vocab, MAX_LEN) for tokens in tokenized_texts]\n",
    "\n",
    "# Split encoded_texts and labels into train, val, test sets (same sizes as before)\n",
    "encoded_texts = np.array(encoded_texts)\n",
    "labels = np.array(labels)\n",
    "\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    encoded_texts, labels, test_size=TEST_SIZE, random_state=RANDOM_SEED, shuffle=True\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=VAL_SIZE, random_state=RANDOM_SEED, shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"LSTM splits - Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
    "\n",
    "# 2. Create Dataset\n",
    "train_dataset_lstm = ReviewDataset(X_train, y_train)\n",
    "val_dataset_lstm = ReviewDataset(X_val, y_val)\n",
    "test_dataset_lstm = ReviewDataset(X_test, y_test)\n",
    "\n",
    "# 3. Create DataLoaders\n",
    "train_loader_lstm = DataLoader(train_dataset_lstm, batch_size=16, shuffle=True)\n",
    "val_loader_lstm = DataLoader(val_dataset_lstm, batch_size=32)\n",
    "test_loader_lstm = DataLoader(test_dataset_lstm, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75022e68",
   "metadata": {},
   "source": [
    "# Transformer based Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80f0f12",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01338243",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SA_Model():\n",
    "    def __init__(self, model_name=\"roberta-base\", num_labels=5):\n",
    "        self.model_name = model_name\n",
    "        self.num_labels = num_labels\n",
    "        self.model = RobertaForSequenceClassification.from_pretrained(self.model_name, num_labels=self.num_labels)\n",
    "        \n",
    "    def apply_lora(self, rank=16):\n",
    "        lora_config = LoraConfig(\n",
    "            r=rank,\n",
    "            lora_alpha=32,\n",
    "            lora_dropout=0.1,\n",
    "            bias=\"none\",\n",
    "            task_type=\"SEQ_CLS\",  # For sequence classification\n",
    "            target_modules=[\"query\", \"value\"],\n",
    "        )\n",
    "        self.model = get_peft_model(self.model, lora_config)\n",
    "        \n",
    "    def freeze_all_except_lora(self):\n",
    "        \"\"\"Freeze all base model weights except LoRA adapters\"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if \"lora\" not in name:\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def unfreeze_classifier(self):\n",
    "        \"\"\"Unfreeze the classifier layer for fine-tuning\"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if \"classifier\" in name:\n",
    "                param.requires_grad = True\n",
    "\n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "    \n",
    "    def to_device(self, device):\n",
    "        \"\"\"Move the model to the specified device (CPU or GPU)\"\"\"\n",
    "        self.model.to(device)\n",
    "        return self\n",
    "    \n",
    "    def get_trainable_parameters(self):\n",
    "        # for name, param in self.model.named_parameters():\n",
    "        #     if param.requires_grad:\n",
    "        #         print(\"Trainable:\", name)\n",
    "        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        total_params = sum(p.numel() for p in self.model.parameters())\n",
    "        print(f\"Trainable parameters: {trainable_params:,} / {total_params:,} ({100 * trainable_params / total_params:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65c5563",
   "metadata": {},
   "source": [
    "## Model evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2000a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k != \"labels\"}\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits if hasattr(outputs, \"logits\") else outputs[0]\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(all_labels, all_preds))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    disp.plot(cmap='Blues', ax=ax)\n",
    "    plt.savefig(RESULTS_DIR + \"/transformer/confusion_matrix_transformer.png\")\n",
    "    plt.show()\n",
    "    return accuracy, precision, recall, f1, cm\n",
    "\n",
    "def evaluate_model_as_sentiment(model, data_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k != \"labels\"}\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits if hasattr(outputs, \"logits\") else outputs[0]\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    all_preds = [stars_to_sentiment(pred) for pred in all_preds]\n",
    "    all_labels = [stars_to_sentiment(label) for label in all_labels]\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(all_labels, all_preds))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Negative\", \"Neutral\", \"Positive\"])\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    disp.plot(cmap='Blues', ax=ax)\n",
    "    plt.savefig(RESULTS_DIR + \"/transformer/confusion_matrix_transformer_sentiment.png\")\n",
    "    plt.show()\n",
    "    return accuracy, precision, recall, f1, cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9608b17c",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bfc24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SA_Model()\n",
    "model.apply_lora(rank=32)\n",
    "model.freeze_all_except_lora() \n",
    "model.unfreeze_classifier()\n",
    "model.get_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7cd15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=RESULTS_DIR + \"/transformer\",    # Where checkpoints/logs go\n",
    "    per_device_train_batch_size=16,             # Batch size per GPU\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,                        # Total number of training epochs\n",
    "    learning_rate=3e-5,\n",
    "    warmup_ratio=0.1,                           # 10% of training steps for warmup\n",
    "    eval_strategy=\"epoch\",                      # Run eval at end of every epoch\n",
    "    save_strategy=\"epoch\",                      # Save checkpoint every epoch\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    fp16=True,                           \n",
    "    save_total_limit=5,                         # Only keep the last 5 checkpoints\n",
    "    load_best_model_at_end=True,                # Use best checkpoint (based on metric)\n",
    "    metric_for_best_model=\"f1\",                 # Choose best model by accuracy\n",
    "    greater_is_better=True,\n",
    "    weight_decay=0.01,                          # Weight decay for regularization\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model.get_model(),\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]  \n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d2ffa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.save_pretrained(f\"{RESULTS_DIR}/transformer/\")\n",
    "tokenizer.save_pretrained(f\"{RESULTS_DIR}/transformer/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af540b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from the saved directory\n",
    "model = SA_Model(f\"{RESULTS_DIR}/transformer/\", num_labels=5)\n",
    "model.freeze_all_except_lora()\n",
    "model.to_device(device)\n",
    "\n",
    "evaluate_model(model.get_model(), test_loader_bert)\n",
    "evaluate_model_as_sentiment(model.get_model(), test_loader_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7efb3ca",
   "metadata": {},
   "source": [
    "# LSTM based Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1c8b37",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a215f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        # Take mean over all hidden states (sequence dimension)\n",
    "        mean_hidden = lstm_out.mean(dim=1)\n",
    "        out = self.dropout(mean_hidden)\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3631bb",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b497987",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(all_labels, all_preds))\n",
    "    \n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    disp.plot(cmap='Blues', ax=ax)\n",
    "    plt.savefig(RESULTS_DIR + \"/lstm/confusion_matrix.png\")\n",
    "    plt.plot()\n",
    "    \n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "def evaluate_model_as_sentiment(model, data_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    all_preds = [stars_to_sentiment(pred) for pred in all_preds]\n",
    "    all_labels = [stars_to_sentiment(label) for label in all_labels]\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(all_labels, all_preds))\n",
    "    \n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Negative\", \"Neutral\", \"Positive\"])\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    disp.plot(cmap='Blues', ax=ax)\n",
    "    plt.savefig(RESULTS_DIR + \"/lstm/confusion_matrix_sentiment.png\")\n",
    "    plt.plot()\n",
    "    \n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=100, plot=False, patience=10, min_epochs=0, \n",
    "                learning_rate=0.0003, weight_decay=1e-5):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    f_scores = []\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Training Loss: {total_loss / len(train_loader)}\")\n",
    "        train_losses.append(total_loss / len(train_loader))\n",
    "\n",
    "        # Evaluate on validation set every epoch\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        labels_list = []\n",
    "        outputs_list = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                labels_list.extend(labels.cpu().numpy())\n",
    "                outputs_list.extend(predicted.cpu().numpy())\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        \n",
    "        val_accuracy = val_correct / val_total\n",
    "        val_loss_avg = val_loss / len(val_loader)\n",
    "        val_losses.append(val_loss_avg)\n",
    "        prec, rec, f_score, _ = precision_recall_fscore_support(labels_list, outputs_list, average='weighted')\n",
    "        f_scores.append(f_score)\n",
    "        print(f\"Validation Loss: {val_loss_avg:.4f},\")\n",
    "        print(f\"Validation Accuracy: {val_accuracy:.4f},\")\n",
    "        print(f\"Precision: {prec:.4f},\")\n",
    "        print(f\"Recall: {rec:.4f},\")\n",
    "        print(f\"F1 Score: {f_score:.4f}\")\n",
    "        model.train()\n",
    "\n",
    "        # Early stoppin\n",
    "        if epoch == 0:\n",
    "            best_f_score = f_score\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            if f_score > best_f_score:\n",
    "                best_f_score = f_score\n",
    "                epochs_without_improvement = 0\n",
    "                # Save best model\n",
    "                torch.save(model.state_dict(), RESULTS_DIR + '/lstm/best_model.pth')\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "                \n",
    "            if epochs_without_improvement >= patience and epoch >= min_epochs:\n",
    "                print(f\"Early stopping triggered after {epoch + 1} epochs\")\n",
    "                break\n",
    "\n",
    "    # Plot training loss\n",
    "    if plot:\n",
    "        plt.figure()\n",
    "        plt.plot(range(1, epoch + 2), train_losses, label='Training Loss')\n",
    "        plt.plot(range(1, epoch + 2), val_losses, label='Validation Loss')\n",
    "        plt.plot(range(1, epoch + 2), f_scores, label='Validation F1 Score')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training Loss Curve')\n",
    "        plt.legend()\n",
    "        plt.savefig(RESULTS_DIR + \"/lstm/loss.png\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75979015",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7bd32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMClassifier(vocab_size=len(vocab), embed_dim=128, hidden_dim=128, output_dim=5)\n",
    "model.to(device)\n",
    "\n",
    "# Train the LSTM model\n",
    "train_model(model, train_loader_lstm, val_loader_lstm, \n",
    "            num_epochs=100, plot=True, learning_rate=0.0001,\n",
    "            patience=10, min_epochs=0, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e742ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model, test_loader_lstm)\n",
    "evaluate_model_as_sentiment(model, test_loader_lstm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
